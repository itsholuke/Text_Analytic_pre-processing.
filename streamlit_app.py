# pages/1_preprocessing.py
# --------------------------------------------------------------------
# Preâ€‘processing (SentenceÂ TokenizerÂ +Â RollingÂ Context)
# --------------------------------------------------------------------
import re
import pandas as pd
import streamlit as st

# ---------- sentence tokenizer (NLTK if present, regex fallback) ----------
try:
    from nltk.tokenize import sent_tokenize  # requires Punkt once downloaded
except ModuleNotFoundError:
    def sent_tokenize(text: str):
        return [s.strip() for s in re.split(r"(?<=[.!?])\s+", str(text).strip()) if s.strip()]
# -------------------------------------------------------------------------

st.set_page_config(page_title="Pre-processing (Tokenizer & Rolling Context)", page_icon="âœ‚ï¸")
st.title("âœ‚ï¸ Pre-processing (SentenceÂ TokenizerÂ +Â RollingÂ Context)")

st.markdown(
    """
    **StepÂ 1.** Upload any CSV and map the *Postâ€‘ID* and *Caption/Text* columns.  \ 
    **StepÂ 2.** Iâ€™ll split each caption into individual sentences and let you
    download the tokenised file.\

    **StepÂ 3 (optional).** Click **Add Rolling Context** to create a twoâ€‘sentence
    rolling window column and download the extended file.
    """
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1 â€¢ Upload
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
upload = st.file_uploader("ðŸ“ Upload CSV", type="csv")
if upload is None:
    st.stop()

raw_df = pd.read_csv(upload)
st.subheader("Preview of uploaded data")
st.dataframe(raw_df.head(), use_container_width=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2 â€¢ Column mapping
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cols = raw_df.columns.tolist()

id_col   = st.selectbox("ðŸ†” Column that uniquely identifies each post", cols)
text_col = st.selectbox("ðŸ“ Column that contains the full caption / text", cols)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3 â€¢ Tokenise
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if st.button("ðŸš€ Run Preâ€‘processing"):
    sent_rows = []
    for _, row in raw_df.iterrows():
        post_id = row[id_col]
        caption = str(row[text_col])
        sentences = sent_tokenize(caption)
        for idx, sent in enumerate(sentences, start=1):
            sent_rows.append({
                "ID":          post_id,
                "Context":     caption,
                "Statement":   sent,
                "Sentence ID": idx,
            })

    token_df = pd.DataFrame(sent_rows)
    st.session_state["token_df"] = token_df  # store for later

    st.success(f"Tokenised {len(raw_df):,} posts into {len(token_df):,} sentences.")
    st.subheader("Tokenised output (first 10 rows)")
    st.dataframe(token_df.head(10), use_container_width=True)

    st.download_button(
        "ðŸ“¥ Download tokenised CSV",
        token_df.to_csv(index=False).encode(),
        "ig_posts_tokenised.csv",
        "text/csv",
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4 â€¢ Optional Rolling Context
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if st.session_state.get("token_df") is not None:
    st.divider()
    st.header("Optional: Add Rolling Context")
    if st.button("âž• Add twoâ€‘sentence rolling context and download"):
        token_df = st.session_state["token_df"].copy()
        token_df["Rolling_Context"] = token_df.groupby("ID")["Statement"].apply(
            lambda s: s.shift(1).fillna("") + " " + s
        )
        token_df["Rolling_Context"] = token_df["Rolling_Context"].str.strip()

        st.success("Rolling context column added.")
        st.dataframe(token_df.head(10), use_container_width=True)

        st.download_button(
            "ðŸ“¥ Download tokenised + context CSV",
            token_df.to_csv(index=False).encode(),
            "ig_posts_tokenised_with_context.csv",
            "text/csv",
        )
